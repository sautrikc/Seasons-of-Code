{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwV8exBkq4tVSs6TWIosWK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sautrikc/Seasons-of-Code/blob/main/My_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2D CNN"
      ],
      "metadata": {
        "id": "9yNRUxV4k---"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Importing important libraries"
      ],
      "metadata": {
        "id": "TPHPxKlnY4F2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC4WJSaqQpvP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error"
      ],
      "metadata": {
        "id": "9pqqy6bkQqhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Preprocessing"
      ],
      "metadata": {
        "id": "vWDr-ChWZA74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into training and testing set. Further, the training set is divided into training and validation."
      ],
      "metadata": {
        "id": "39xOXkSVcBB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DATADIR = \"./Dataset\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75"
      ],
      "metadata": {
        "id": "FVov7ydcRuJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is organised into dictionaries with the keys of the dictionary set to the name of the stock market index. Further the data is scaled using 'Standard Scaler'."
      ],
      "metadata": {
        "id": "dveDqOlMY3DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {}\n",
        "for filename in os.listdir():\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    #filepath = os.path.join(DATADIR, filename)\n",
        "    X = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X"
      ],
      "metadata": {
        "id": "DXvbymH0Y1ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmo-sWhsc1A2",
        "outputId": "03af5335-a5c3-4aca-b2a9-9aadf0b7b8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['NASDAQ', 'NYA', 'S&P', 'RUT', 'DJI'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A data generator is formed which can produce batches of data when called by our model."
      ],
      "metadata": {
        "id": "nEZ0-absliUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one dataframe from the pool\n",
        "        key = random.choice(list(data.keys()))\n",
        "        df = data[key]\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "        if kind == 'train':\n",
        "            index = index[:split]   # range for the training set\n",
        "        elif kind == 'valid':\n",
        "            index = index[split:]   # range for the validation set\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)      # pick one time step\n",
        "            n = (df.index == t).argmax()  # find its position in the dataframe\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # can't get enough data for one sequence length\n",
        "            frame = df.iloc[n-seq_len+1:n+1]\n",
        "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "            yield X, y\n",
        "            batch = []"
      ],
      "metadata": {
        "id": "bKhyYJIMli35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "talq0Oytacz0",
        "outputId": "f3924d0e-66bb-4195-c16f-4d10170f3d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'Processed_NASDAQ.csv',\n",
              " 'Processed_NYSE.csv',\n",
              " 'Processed_S&P.csv',\n",
              " 'Processed_RUSSELL.csv',\n",
              " 'Processed_DJI.csv',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Model Formulation"
      ],
      "metadata": {
        "id": "h9pWg171koFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model from the paper is implemented. It contains 2 convolution layers with number of filters = 8. The kernel size are (1x82) and (3x1). This is followed by a max pooling layer which reduces the dimension by half. It is followed by another convolution layer of kernel size = (3 x 1) and a Max Pooling layer. Then it is flattened and fed to a Dense layer which returns the output."
      ],
      "metadata": {
        "id": "zFMBE2zAdZ3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(10,10,10,10), droprate=0.2):\n",
        "    \"2D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_len, n_features, 1)),\n",
        "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
        "        Conv2D(n_filters[1], kernel_size=(4,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Conv2D(n_filters[2], kernel_size=(4,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Conv2D(n_filters[3], kernel_size=(4,1), activation=\"relu\"),\n",
        "        Flatten(),\n",
        "        Dropout(droprate),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "xONMHr7kaxXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metrics to evaluate our model are F1 score and accuracy. The mean absolute error is used as the loss function."
      ],
      "metadata": {
        "id": "UpiTfC5GfR00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2"
      ],
      "metadata": {
        "id": "cWbke25CGwXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Model Training"
      ],
      "metadata": {
        "id": "rYyO2AhckuoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 60\n",
        "batch_size = 128\n",
        "n_epochs = 25\n",
        "n_features = 82\n",
        " \n",
        "# Produce CNNpred as a binary classification problem\n",
        "model = cnnpred_2d(seq_len, n_features)\n",
        "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
        "model.summary()  # print model structure to console\n",
        " \n",
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\n",
        "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16p2PTH4ISQk",
        "outputId": "21237efa-2bdf-4fd5-8025-875c38ef44ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 60, 1, 10)         830       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 57, 1, 10)         410       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 28, 1, 10)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 25, 1, 10)         410       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 1, 10)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 9, 1, 10)          410       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 90)                0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 90)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 91        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,151\n",
            "Trainable params: 2,151\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "400/400 [==============================] - 71s 174ms/step - loss: 0.4447 - acc: 0.5591 - f1macro: 0.3688 - val_loss: 0.4752 - val_acc: 0.5250 - val_f1macro: 0.3439\n",
            "Epoch 2/25\n",
            "400/400 [==============================] - 74s 186ms/step - loss: 0.3151 - acc: 0.6991 - f1macro: 0.6432 - val_loss: 0.5147 - val_acc: 0.4727 - val_f1macro: 0.4511\n",
            "Epoch 3/25\n",
            "400/400 [==============================] - 72s 180ms/step - loss: 0.1840 - acc: 0.8261 - f1macro: 0.8176 - val_loss: 0.4813 - val_acc: 0.5227 - val_f1macro: 0.5108\n",
            "Epoch 4/25\n",
            "400/400 [==============================] - 65s 162ms/step - loss: 0.1614 - acc: 0.8445 - f1macro: 0.8373 - val_loss: 0.4617 - val_acc: 0.5477 - val_f1macro: 0.5379\n",
            "Epoch 5/25\n",
            "400/400 [==============================] - 66s 165ms/step - loss: 0.1533 - acc: 0.8508 - f1macro: 0.8440 - val_loss: 0.4820 - val_acc: 0.5148 - val_f1macro: 0.5052\n",
            "Epoch 6/25\n",
            "400/400 [==============================] - 65s 163ms/step - loss: 0.1423 - acc: 0.8616 - f1macro: 0.8562 - val_loss: 0.4754 - val_acc: 0.5320 - val_f1macro: 0.5191\n",
            "Epoch 7/25\n",
            "400/400 [==============================] - 65s 163ms/step - loss: 0.1382 - acc: 0.8649 - f1macro: 0.8597 - val_loss: 0.4509 - val_acc: 0.5547 - val_f1macro: 0.5335\n",
            "Epoch 8/25\n",
            "400/400 [==============================] - 64s 161ms/step - loss: 0.1393 - acc: 0.8628 - f1macro: 0.8579 - val_loss: 0.4860 - val_acc: 0.5117 - val_f1macro: 0.5009\n",
            "Epoch 9/25\n",
            "400/400 [==============================] - 65s 162ms/step - loss: 0.1358 - acc: 0.8662 - f1macro: 0.8610 - val_loss: 0.4707 - val_acc: 0.5336 - val_f1macro: 0.5181\n",
            "Epoch 10/25\n",
            "400/400 [==============================] - 63s 159ms/step - loss: 0.1391 - acc: 0.8627 - f1macro: 0.8574 - val_loss: 0.4980 - val_acc: 0.5023 - val_f1macro: 0.4873\n",
            "Epoch 11/25\n",
            "400/400 [==============================] - 65s 162ms/step - loss: 0.1371 - acc: 0.8644 - f1macro: 0.8593 - val_loss: 0.4823 - val_acc: 0.5195 - val_f1macro: 0.5059\n",
            "Epoch 12/25\n",
            "400/400 [==============================] - 64s 161ms/step - loss: 0.1308 - acc: 0.8707 - f1macro: 0.8658 - val_loss: 0.4831 - val_acc: 0.5141 - val_f1macro: 0.5016\n",
            "Epoch 13/25\n",
            "400/400 [==============================] - 64s 160ms/step - loss: 0.1322 - acc: 0.8694 - f1macro: 0.8645 - val_loss: 0.4711 - val_acc: 0.5281 - val_f1macro: 0.5151\n",
            "Epoch 14/25\n",
            "400/400 [==============================] - 63s 159ms/step - loss: 0.1285 - acc: 0.8729 - f1macro: 0.8683 - val_loss: 0.4927 - val_acc: 0.5047 - val_f1macro: 0.4971\n",
            "Epoch 15/25\n",
            "400/400 [==============================] - 63s 159ms/step - loss: 0.1289 - acc: 0.8725 - f1macro: 0.8678 - val_loss: 0.4911 - val_acc: 0.5070 - val_f1macro: 0.4970\n",
            "Epoch 16/25\n",
            "400/400 [==============================] - 63s 158ms/step - loss: 0.1259 - acc: 0.8752 - f1macro: 0.8706 - val_loss: 0.4791 - val_acc: 0.5219 - val_f1macro: 0.5042\n",
            "Epoch 17/25\n",
            "400/400 [==============================] - 63s 157ms/step - loss: 0.1232 - acc: 0.8776 - f1macro: 0.8733 - val_loss: 0.4727 - val_acc: 0.5305 - val_f1macro: 0.5203\n",
            "Epoch 18/25\n",
            "400/400 [==============================] - 64s 160ms/step - loss: 0.1230 - acc: 0.8780 - f1macro: 0.8734 - val_loss: 0.4582 - val_acc: 0.5398 - val_f1macro: 0.5274\n",
            "Epoch 19/25\n",
            "400/400 [==============================] - 63s 157ms/step - loss: 0.1234 - acc: 0.8775 - f1macro: 0.8733 - val_loss: 0.4918 - val_acc: 0.5086 - val_f1macro: 0.4930\n",
            "Epoch 20/25\n",
            "400/400 [==============================] - 64s 160ms/step - loss: 0.1229 - acc: 0.8781 - f1macro: 0.8736 - val_loss: 0.4589 - val_acc: 0.5391 - val_f1macro: 0.5260\n",
            "Epoch 21/25\n",
            "400/400 [==============================] - 62s 156ms/step - loss: 0.1180 - acc: 0.8827 - f1macro: 0.8785 - val_loss: 0.4640 - val_acc: 0.5383 - val_f1macro: 0.5204\n",
            "Epoch 22/25\n",
            "400/400 [==============================] - 63s 159ms/step - loss: 0.1204 - acc: 0.8805 - f1macro: 0.8764 - val_loss: 0.4456 - val_acc: 0.5555 - val_f1macro: 0.5417\n",
            "Epoch 23/25\n",
            "400/400 [==============================] - 64s 159ms/step - loss: 0.1195 - acc: 0.8815 - f1macro: 0.8768 - val_loss: 0.4668 - val_acc: 0.5352 - val_f1macro: 0.5227\n",
            "Epoch 24/25\n",
            "400/400 [==============================] - 64s 161ms/step - loss: 0.1158 - acc: 0.8851 - f1macro: 0.8810 - val_loss: 0.4421 - val_acc: 0.5586 - val_f1macro: 0.5467\n",
            "Epoch 25/25\n",
            "400/400 [==============================] - 65s 163ms/step - loss: 0.1218 - acc: 0.8790 - f1macro: 0.8744 - val_loss: 0.4523 - val_acc: 0.5484 - val_f1macro: 0.5336\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f78c6264f90>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A data generator for the test set is also prepared. The mean absolute error, accuracy and F1 score are measured and reported below."
      ],
      "metadata": {
        "id": "UYGBtI_igDLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Model Testing"
      ],
      "metadata": {
        "id": "tjtf9Tcukzo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testgen(data, seq_len, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    batch = []\n",
        "    for key, df in data.items():\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        # find the start of test sample\n",
        "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
        "        n = (df.index == t).argmax()\n",
        "        for i in range(n+1, len(df)+1):\n",
        "            frame = df.iloc[i-seq_len:i]\n",
        "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
        "    X, y = zip(*batch)\n",
        "    return np.expand_dims(np.array(X),3), np.array(y)\n",
        "\n",
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
        "\n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhNM5L20KMKe",
        "outputId": "2c1dc683-5a4c-418a-9782-22579a75045a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5160975609756098\n",
            "MAE: 0.48390243902439023\n",
            "F1: 0.6248108925869894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
        " \n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBUh7vjwKIle",
        "outputId": "e5b3904a-c200-4525-8e50-86a3d333bc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5160975609756098\n",
            "MAE: 0.48390243902439023\n",
            "F1: 0.6248108925869894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3D CNN"
      ],
      "metadata": {
        "id": "KGNR8Sr1LJmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The paper also mentioned another method of predicting the change in the stock index. It uses data from all the 5 markets to predict the change in stock index for a particular market. The code is presented below."
      ],
      "metadata": {
        "id": "u3UJ-svkgse8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
        "\n",
        "DATADIR = \"./Dataset\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def cnnpred_3d(seq_len=60, n_stocks=5, n_features=82, n_filters=(8,8,8,8), droprate=0.2):\n",
        "    \"3D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(n_stocks, seq_len, n_features)),\n",
        "        Conv2D(n_filters[0], kernel_size=(1,1), activation=\"relu\", data_format=\"channels_last\"),\n",
        "        Conv2D(n_filters[1], kernel_size=(n_stocks,4), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(1,2)),\n",
        "        Conv2D(n_filters[2], kernel_size=(1,4), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(1,2)),\n",
        "        Conv2D(n_filters[3], kernel_size=(1,4), activation=\"relu\"),\n",
        "        Flatten(),\n",
        "        Dropout(droprate),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def datagen(data, seq_len, batch_size, target_index, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    # Learn about the data's features and time axis\n",
        "    input_cols = [c for c in data.columns if c[0] != targetcol]\n",
        "    tickers = sorted(set(c for _,c in input_cols))\n",
        "    n_features = len(input_cols) // len(tickers)\n",
        "    index = data.index[data.index < TRAIN_TEST_CUTOFF]\n",
        "    split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "    assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
        "    if kind == \"train\":\n",
        "        index = index[:split]   # range for the training set\n",
        "    elif kind == 'valid':\n",
        "        index = index[split:]   # range for the validation set\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    # Infinite loop to generate a batch\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)\n",
        "            n = (data.index == t).argmax()\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # this sample is not enough for one sequence length\n",
        "            frame = data.iloc[n-seq_len+1:n+1][input_cols]\n",
        "            # convert frame with two level of indices into 3D array\n",
        "            shape = (len(tickers), len(frame), n_features)\n",
        "            X = np.full(shape, np.nan)\n",
        "            for i,ticker in enumerate(tickers):\n",
        "                X[i] = frame.xs(ticker, axis=1, level=1).values\n",
        "            batch.append([X, data[targetcol][target_index][t]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            yield np.array(X), np.array(y)\n",
        "            batch = []\n",
        "\n",
        "def testgen(data, seq_len, target_index, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    input_cols = [c for c in data.columns if c[0] != targetcol]\n",
        "    tickers = sorted(set(c for _,c in input_cols))\n",
        "    n_features = len(input_cols) // len(tickers)\n",
        "    t = data.index[data.index >= TRAIN_TEST_CUTOFF][0]\n",
        "    n = (data.index == t).argmax()\n",
        "    batch = []\n",
        "    for i in range(n+1, len(data)+1):\n",
        "        # Clip a window of seq_len ends at row position i-1\n",
        "        frame = data.iloc[i-seq_len:i]\n",
        "        target = frame[targetcol][target_index][-1]\n",
        "        frame = frame[input_cols]\n",
        "        # convert frame with two level of indices into 3D array\n",
        "        shape = (len(tickers), len(frame), n_features)\n",
        "        X = np.full(shape, np.nan)\n",
        "        for i,ticker in enumerate(tickers):\n",
        "            X[i] = frame.xs(ticker, axis=1, level=1).values\n",
        "        batch.append([X, target])\n",
        "    X, y = zip(*batch)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir():\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    #filepath = os.path.join(DATADIR, filename)\n",
        "    X = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        "# Transform data into 3D dataframe (multilevel columns)\n",
        "for key, df in data.items():\n",
        "    df.columns = pd.MultiIndex.from_product([df.columns, [key]])\n",
        "data = pd.concat(data.values(), axis=1)\n",
        "\n",
        "seq_len = 60\n",
        "batch_size = 128\n",
        "n_epochs = 25\n",
        "n_features = 82\n",
        "n_stocks = 5\n",
        "\n",
        "# Produce CNNpred as a binary classification problem\n",
        "model = cnnpred_3d(seq_len, n_stocks, n_features)\n",
        "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
        "model.summary() # print model structure to console\n",
        "\n",
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp3d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\n",
        "\n",
        "model.fit(datagen(data, seq_len, batch_size, \"DJI\", \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"DJI\", \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n",
        "\n",
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"DJI\", \"Target\")\n",
        "\n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXMT8H00LJMi",
        "outputId": "a2e3ea3e-4a42-4b04-9212-49bd6591ea11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 5, 60, 8)          664       \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 1, 57, 8)          1288      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 28, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 1, 25, 8)          264       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 12, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 1, 9, 8)           264       \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 72)                0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 72)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 73        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,553\n",
            "Trainable params: 2,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "400/400 [==============================] - 443s 1s/step - loss: 0.3006 - acc: 0.7245 - f1macro: 0.6785 - val_loss: 0.4245 - val_acc: 0.5695 - val_f1macro: 0.5600\n",
            "Epoch 2/25\n",
            "400/400 [==============================] - 444s 1s/step - loss: 0.1224 - acc: 0.8921 - f1macro: 0.8896 - val_loss: 0.4709 - val_acc: 0.5219 - val_f1macro: 0.5055\n",
            "Epoch 3/25\n",
            "400/400 [==============================] - 452s 1s/step - loss: 0.0811 - acc: 0.9277 - f1macro: 0.9259 - val_loss: 0.4292 - val_acc: 0.5789 - val_f1macro: 0.5716\n",
            "Epoch 4/25\n",
            "400/400 [==============================] - 461s 1s/step - loss: 0.0657 - acc: 0.9396 - f1macro: 0.9381 - val_loss: 0.4392 - val_acc: 0.5602 - val_f1macro: 0.5548\n",
            "Epoch 5/25\n",
            "400/400 [==============================] - 447s 1s/step - loss: 0.0618 - acc: 0.9422 - f1macro: 0.9408 - val_loss: 0.4377 - val_acc: 0.5594 - val_f1macro: 0.5561\n",
            "Epoch 6/25\n",
            "400/400 [==============================] - 450s 1s/step - loss: 0.0559 - acc: 0.9471 - f1macro: 0.9458 - val_loss: 0.4517 - val_acc: 0.5445 - val_f1macro: 0.5387\n",
            "Epoch 7/25\n",
            "400/400 [==============================] - 432s 1s/step - loss: 0.0547 - acc: 0.9475 - f1macro: 0.9460 - val_loss: 0.4492 - val_acc: 0.5469 - val_f1macro: 0.5370\n",
            "Epoch 8/25\n",
            "400/400 [==============================] - 436s 1s/step - loss: 0.0534 - acc: 0.9487 - f1macro: 0.9474 - val_loss: 0.4602 - val_acc: 0.5375 - val_f1macro: 0.5347\n",
            "Epoch 9/25\n",
            "400/400 [==============================] - 449s 1s/step - loss: 0.0516 - acc: 0.9501 - f1macro: 0.9489 - val_loss: 0.4518 - val_acc: 0.5492 - val_f1macro: 0.5461\n",
            "Epoch 10/25\n",
            "400/400 [==============================] - 450s 1s/step - loss: 0.0487 - acc: 0.9527 - f1macro: 0.9516 - val_loss: 0.4501 - val_acc: 0.5539 - val_f1macro: 0.5521\n",
            "Epoch 11/25\n",
            "400/400 [==============================] - 451s 1s/step - loss: 0.0479 - acc: 0.9535 - f1macro: 0.9523 - val_loss: 0.4631 - val_acc: 0.5344 - val_f1macro: 0.5330\n",
            "Epoch 12/25\n",
            "400/400 [==============================] - 455s 1s/step - loss: 0.0464 - acc: 0.9550 - f1macro: 0.9538 - val_loss: 0.4210 - val_acc: 0.5773 - val_f1macro: 0.5748\n",
            "Epoch 13/25\n",
            "400/400 [==============================] - 441s 1s/step - loss: 0.0454 - acc: 0.9559 - f1macro: 0.9548 - val_loss: 0.4452 - val_acc: 0.5617 - val_f1macro: 0.5529\n",
            "Epoch 14/25\n",
            "400/400 [==============================] - 441s 1s/step - loss: 0.0444 - acc: 0.9567 - f1macro: 0.9558 - val_loss: 0.4389 - val_acc: 0.5531 - val_f1macro: 0.5475\n",
            "Epoch 15/25\n",
            "400/400 [==============================] - 438s 1s/step - loss: 0.0448 - acc: 0.9561 - f1macro: 0.9551 - val_loss: 0.4464 - val_acc: 0.5570 - val_f1macro: 0.5495\n",
            "Epoch 16/25\n",
            "400/400 [==============================] - 435s 1s/step - loss: 0.0440 - acc: 0.9571 - f1macro: 0.9560 - val_loss: 0.4849 - val_acc: 0.5094 - val_f1macro: 0.5043\n",
            "Epoch 17/25\n",
            "400/400 [==============================] - 436s 1s/step - loss: 0.0431 - acc: 0.9579 - f1macro: 0.9569 - val_loss: 0.4825 - val_acc: 0.5117 - val_f1macro: 0.5043\n",
            "Epoch 18/25\n",
            "400/400 [==============================] - 435s 1s/step - loss: 0.0426 - acc: 0.9580 - f1macro: 0.9570 - val_loss: 0.4563 - val_acc: 0.5437 - val_f1macro: 0.5408\n",
            "Epoch 19/25\n",
            "400/400 [==============================] - 437s 1s/step - loss: 0.0405 - acc: 0.9604 - f1macro: 0.9595 - val_loss: 0.4156 - val_acc: 0.5875 - val_f1macro: 0.5818\n",
            "Epoch 20/25\n",
            "265/400 [==================>...........] - ETA: 2:31 - loss: 0.0420 - acc: 0.9585 - f1macro: 0.9576"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F1 score is lower in this case and hence the performance of 2D CNN is better."
      ],
      "metadata": {
        "id": "_4VuY-79hSz8"
      }
    }
  ]
}